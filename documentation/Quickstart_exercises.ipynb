{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises created by Qwen2.5-Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulação de Tensores  \n",
    "\n",
    "1.1 Altere o código para carregar um conjunto de dados diferente, como o CIFAR-10 (torchvision.datasets.CIFAR10). <br>\n",
    "1.2 Modifique o tamanho do lote (batch_size) para 32 e observe como isso afeta o tempo de treinamento. <br>\n",
    "1.3 Adicione uma transformação personalizada ao pipeline de dados, como normalizar os valores dos pixels para o intervalo [-1, 1]. <br>\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 3, 32, 32])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# ToTensor transforms from (0, 255) to (0, 1), but i want to transform it to (-1, 1),\n",
    "# which is better to work with activation functions such as Tahn, while sigmoid is better\n",
    "# with (0, 1).\n",
    "\n",
    "# this refers to the value a pixel can take, where (0, 255) represents a normal representation\n",
    "# most images format, where 0 is black and 255 white.\n",
    "\n",
    "# creating a transformation function\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # normalise to [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalise to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# modifying the batch size to to 32 e compare o tempo de treino com 64\n",
    "\n",
    "batch_size_64 = 64\n",
    "batch_size_32 = 32\n",
    "\n",
    "# dataloaders\n",
    "train_dataloader_64 = DataLoader(training_data, batch_size=batch_size_64)\n",
    "train_dataloader_32 = DataLoader(training_data, batch_size=batch_size_32)\n",
    "\n",
    "# find out what the shape necessary for the input\n",
    "for X, y in train_dataloader_64:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch dataloader_64 1\n",
      "--------------\n",
      "loss: 2.304279 [   64/50000]\n",
      "loss: 2.247720 [ 6464/50000]\n",
      "loss: 2.209546 [12864/50000]\n",
      "loss: 2.207372 [19264/50000]\n",
      "loss: 2.126846 [25664/50000]\n",
      "loss: 2.129681 [32064/50000]\n",
      "loss: 2.141927 [38464/50000]\n",
      "loss: 2.054298 [44864/50000]\n",
      "Epoch dataloader_64 2\n",
      "--------------\n",
      "loss: 2.117157 [   64/50000]\n",
      "loss: 2.088054 [ 6464/50000]\n",
      "loss: 1.998731 [12864/50000]\n",
      "loss: 2.090847 [19264/50000]\n",
      "loss: 2.015155 [25664/50000]\n",
      "loss: 2.025780 [32064/50000]\n",
      "loss: 2.085927 [38464/50000]\n",
      "loss: 1.950084 [44864/50000]\n",
      "Epoch dataloader_64 3\n",
      "--------------\n",
      "loss: 2.048022 [   64/50000]\n",
      "loss: 2.014682 [ 6464/50000]\n",
      "loss: 1.869377 [12864/50000]\n",
      "loss: 2.016907 [19264/50000]\n",
      "loss: 1.979053 [25664/50000]\n",
      "loss: 1.973342 [32064/50000]\n",
      "loss: 2.050645 [38464/50000]\n",
      "loss: 1.895323 [44864/50000]\n",
      "Epoch dataloader_64 4\n",
      "--------------\n",
      "loss: 1.996940 [   64/50000]\n",
      "loss: 1.971885 [ 6464/50000]\n",
      "loss: 1.782338 [12864/50000]\n",
      "loss: 1.962134 [19264/50000]\n",
      "loss: 1.957988 [25664/50000]\n",
      "loss: 1.943634 [32064/50000]\n",
      "loss: 2.021637 [38464/50000]\n",
      "loss: 1.862158 [44864/50000]\n",
      "Epoch dataloader_64 5\n",
      "--------------\n",
      "loss: 1.953354 [   64/50000]\n",
      "loss: 1.941664 [ 6464/50000]\n",
      "loss: 1.720632 [12864/50000]\n",
      "loss: 1.921140 [19264/50000]\n",
      "loss: 1.939620 [25664/50000]\n",
      "loss: 1.923103 [32064/50000]\n",
      "loss: 1.999607 [38464/50000]\n",
      "loss: 1.837984 [44864/50000]\n",
      "Epoch dataloader_32 1\n",
      "--------------\n",
      "loss: 1.871489 [   32/50000]\n",
      "loss: 1.899796 [ 3232/50000]\n",
      "loss: 1.806436 [ 6432/50000]\n",
      "loss: 1.803224 [ 9632/50000]\n",
      "loss: 1.621786 [12832/50000]\n",
      "loss: 1.991725 [16032/50000]\n",
      "loss: 1.871554 [19232/50000]\n",
      "loss: 1.843531 [22432/50000]\n",
      "loss: 1.939747 [25632/50000]\n",
      "loss: 1.853088 [28832/50000]\n",
      "loss: 1.797264 [32032/50000]\n",
      "loss: 1.894071 [35232/50000]\n",
      "loss: 2.051963 [38432/50000]\n",
      "loss: 1.790615 [41632/50000]\n",
      "loss: 1.805665 [44832/50000]\n",
      "loss: 2.149505 [48032/50000]\n",
      "Epoch dataloader_32 2\n",
      "--------------\n",
      "loss: 1.831059 [   32/50000]\n",
      "loss: 1.871935 [ 3232/50000]\n",
      "loss: 1.755586 [ 6432/50000]\n",
      "loss: 1.745644 [ 9632/50000]\n",
      "loss: 1.568547 [12832/50000]\n",
      "loss: 1.956726 [16032/50000]\n",
      "loss: 1.837769 [19232/50000]\n",
      "loss: 1.829654 [22432/50000]\n",
      "loss: 1.912233 [25632/50000]\n",
      "loss: 1.790384 [28832/50000]\n",
      "loss: 1.766034 [32032/50000]\n",
      "loss: 1.855969 [35232/50000]\n",
      "loss: 2.036819 [38432/50000]\n",
      "loss: 1.774551 [41632/50000]\n",
      "loss: 1.781211 [44832/50000]\n",
      "loss: 2.163256 [48032/50000]\n",
      "Epoch dataloader_32 3\n",
      "--------------\n",
      "loss: 1.798431 [   32/50000]\n",
      "loss: 1.847592 [ 3232/50000]\n",
      "loss: 1.709920 [ 6432/50000]\n",
      "loss: 1.700543 [ 9632/50000]\n",
      "loss: 1.542421 [12832/50000]\n",
      "loss: 1.927676 [16032/50000]\n",
      "loss: 1.816817 [19232/50000]\n",
      "loss: 1.818787 [22432/50000]\n",
      "loss: 1.885656 [25632/50000]\n",
      "loss: 1.743336 [28832/50000]\n",
      "loss: 1.744806 [32032/50000]\n",
      "loss: 1.828960 [35232/50000]\n",
      "loss: 2.018045 [38432/50000]\n",
      "loss: 1.767603 [41632/50000]\n",
      "loss: 1.759348 [44832/50000]\n",
      "loss: 2.172240 [48032/50000]\n",
      "Epoch dataloader_32 4\n",
      "--------------\n",
      "loss: 1.774379 [   32/50000]\n",
      "loss: 1.827386 [ 3232/50000]\n",
      "loss: 1.674039 [ 6432/50000]\n",
      "loss: 1.663899 [ 9632/50000]\n",
      "loss: 1.526866 [12832/50000]\n",
      "loss: 1.901399 [16032/50000]\n",
      "loss: 1.801686 [19232/50000]\n",
      "loss: 1.806680 [22432/50000]\n",
      "loss: 1.861184 [25632/50000]\n",
      "loss: 1.706084 [28832/50000]\n",
      "loss: 1.729850 [32032/50000]\n",
      "loss: 1.810899 [35232/50000]\n",
      "loss: 1.997375 [38432/50000]\n",
      "loss: 1.762854 [41632/50000]\n",
      "loss: 1.740785 [44832/50000]\n",
      "loss: 2.178128 [48032/50000]\n",
      "Epoch dataloader_32 5\n",
      "--------------\n",
      "loss: 1.756598 [   32/50000]\n",
      "loss: 1.811209 [ 3232/50000]\n",
      "loss: 1.647132 [ 6432/50000]\n",
      "loss: 1.633605 [ 9632/50000]\n",
      "loss: 1.515653 [12832/50000]\n",
      "loss: 1.876679 [16032/50000]\n",
      "loss: 1.789294 [19232/50000]\n",
      "loss: 1.794412 [22432/50000]\n",
      "loss: 1.839787 [25632/50000]\n",
      "loss: 1.675753 [28832/50000]\n",
      "loss: 1.718022 [32032/50000]\n",
      "loss: 1.799326 [35232/50000]\n",
      "loss: 1.977217 [38432/50000]\n",
      "loss: 1.757149 [41632/50000]\n",
      "loss: 1.725364 [44832/50000]\n",
      "loss: 2.181412 [48032/50000]\n",
      "Time elapsed for batch_size of 64:53.771392\n",
      "Time elapsed for batch_size of 32:61.026029\n"
     ]
    }
   ],
   "source": [
    "# making sure it runs on gpu\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "# defining model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3*32*32, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = SimpleNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# train loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Computer prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# training loop with batch_size 64\n",
    "start_test1 = time.time() # save initial time\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch dataloader_64 {t+1}\\n--------------\")\n",
    "    train(train_dataloader_64, model, loss_fn, optimizer)\n",
    "end_test1 = time.time() # save end time\n",
    "\n",
    "# training loop with batch_size 32\n",
    "start_test2 = time.time()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch dataloader_32 {t+1}\\n--------------\")\n",
    "    train(train_dataloader_32, model, loss_fn, optimizer)\n",
    "end_test2 = time.time()\n",
    "\n",
    "print(f\"Time elapsed for batch_size of 64:{end_test1-start_test1:5f}\")\n",
    "print(f\"Time elapsed for batch_size of 32:{end_test2-start_test2:5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição do Modelo  \n",
    "\n",
    "2.1 Altere o modelo para incluir mais uma camada oculta com 128 neurônios e uma função de ativação ReLU. <br>\n",
    "2.2 Substitua a função de ativação ReLU por outra, como LeakyReLU ou Tanh. Compare os resultados. <br>\n",
    "2.3 Implemente um modelo usando uma arquitetura convolucional (CNN) em vez de uma rede totalmente conectada. Use pelo menos duas camadas convolucionais seguidas por camadas de pooling. <br>\n",
    "\n",
    "Treinamento do Modelo  \n",
    "\n",
    "3.1 Altere o número de épocas para 10 e observe como isso afeta a precisão do modelo.<br>\n",
    "3.2 Adicione um mecanismo de parada antecipada (early stopping) que interrompe o treinamento se a perda de validação não melhorar por 3 épocas consecutivas. <br>\n",
    "3.3 Experimente diferentes funções de perda, como nn.CrossEntropyLoss e nn.NLLLoss. Explique as diferenças entre elas e como elas afetam o treinamento. <br>\n",
    "3.4 Implemente um otimizador diferente, como Adam (torch.optim.Adam), e compare seu desempenho com o SGD. <br>\n",
    "\n",
    "Avaliação do Modelo  \n",
    "\n",
    "4.1 Calcule outras métricas de avaliação, como recall, precisão e F1-score, para avaliar o desempenho do modelo. <br>\n",
    "4.2 Plote a matriz de confusão para visualizar os erros do modelo. <br>\n",
    "4.3 Salve o modelo treinado usando torch.save e carregue-o novamente usando torch.load. <br>\n",
    "4.4 Teste o modelo carregado em novos dados. <br>\n",
    "\n",
    "Experimentação e Análise  \n",
    "\n",
    "5.1 Realize experimentos com diferentes taxas de aprendizado (learning rate). Qual é a taxa ideal para o problema? <br>\n",
    "5.2 Aplique regularização L2 (weight decay) ao otimizador e observe como isso afeta o desempenho do modelo. <br>\n",
    "5.3 Use dropout (nn.Dropout) no modelo para reduzir o overfitting. Teste diferentes valores de probabilidade de dropout (ex.: 0.2, 0.5). <br>\n",
    "5.4 Experimente diferentes inicializações de pesos para as camadas do modelo (ex.: torch.nn.init.xavier_uniform_). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desafio Final  \n",
    "\n",
    "Combine tudo o que aprendeu para resolver um problema mais complexo. \n",
    "\n",
    "1.1 Escolha um novo conjunto de dados (por exemplo, MNIST, CIFAR-10 ou qualquer outro disponível no torchvision.datasets). <br>\n",
    "1.2 Projete e implemente um modelo CNN personalizado para classificar as imagens desse conjunto de dados. <br>\n",
    "1.3 Treine o modelo, ajuste os hiperparâmetros e avalie seu desempenho. <br>\n",
    "1.4 Documente suas descobertas e explique quais decisões de design contribuíram para o sucesso do modelo. <br>\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
