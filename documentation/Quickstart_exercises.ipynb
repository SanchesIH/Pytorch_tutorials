{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises created by Qwen2.5-Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Manipulation\n",
    "\n",
    "1.1 Modify the code to load a different dataset, such as CIFAR-10 (torchvision.datasets.CIFAR10). <br>\n",
    "1.2 Change the batch size (batch_size) to 32 and observe how this affects the training time. <br>\n",
    "1.3 Add a custom transformation to the data pipeline, such as normalizing pixel values to the range [-1, 1]. <br>\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 3, 32, 32])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# ToTensor transforms from (0, 255) to (0, 1), but i want to transform it to (-1, 1),\n",
    "# which is better to work with activation functions such as Tahn, while sigmoid is better\n",
    "# with (0, 1).\n",
    "\n",
    "# this refers to the value a pixel can take, where (0, 255) represents a normal representation\n",
    "# most images format, where 0 is black and 255 white.\n",
    "\n",
    "# creating a transformation function\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # normalise to [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # normalise to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# modifying the batch size to to 32 e compare o tempo de treino com 64\n",
    "\n",
    "batch_size_64 = 64\n",
    "batch_size_32 = 32\n",
    "\n",
    "# train dataloaders\n",
    "train_dataloader_64 = DataLoader(training_data, batch_size=batch_size_64)\n",
    "train_dataloader_32 = DataLoader(training_data, batch_size=batch_size_32)\n",
    "\n",
    "# test dataloaders\n",
    "test_dataloader_64 = DataLoader(test_data, batch_size=batch_size_64)\n",
    "test_dataloader_32 = DataLoader(test_data, batch_size=batch_size_32)\n",
    "\n",
    "# find out what the shape necessary for the input\n",
    "for X, y in train_dataloader_64:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch dataloader_64 1\n",
      "--------------\n",
      "loss: 2.293588 [   64/50000]\n",
      "loss: 2.296496 [ 6464/50000]\n",
      "loss: 2.285605 [12864/50000]\n",
      "loss: 2.276576 [19264/50000]\n",
      "loss: 2.282355 [25664/50000]\n",
      "loss: 2.269580 [32064/50000]\n",
      "loss: 2.270891 [38464/50000]\n",
      "loss: 2.246101 [44864/50000]\n",
      "Epoch dataloader_64 2\n",
      "--------------\n",
      "loss: 2.246453 [   64/50000]\n",
      "loss: 2.232383 [ 6464/50000]\n",
      "loss: 2.214886 [12864/50000]\n",
      "loss: 2.229003 [19264/50000]\n",
      "loss: 2.201321 [25664/50000]\n",
      "loss: 2.203708 [32064/50000]\n",
      "loss: 2.204005 [38464/50000]\n",
      "loss: 2.163888 [44864/50000]\n",
      "Epoch dataloader_64 3\n",
      "--------------\n",
      "loss: 2.193636 [   64/50000]\n",
      "loss: 2.162617 [ 6464/50000]\n",
      "loss: 2.128359 [12864/50000]\n",
      "loss: 2.175662 [19264/50000]\n",
      "loss: 2.113619 [25664/50000]\n",
      "loss: 2.135772 [32064/50000]\n",
      "loss: 2.143578 [38464/50000]\n",
      "loss: 2.080114 [44864/50000]\n",
      "Epoch dataloader_64 4\n",
      "--------------\n",
      "loss: 2.138625 [   64/50000]\n",
      "loss: 2.098900 [ 6464/50000]\n",
      "loss: 2.045592 [12864/50000]\n",
      "loss: 2.126187 [19264/50000]\n",
      "loss: 2.043897 [25664/50000]\n",
      "loss: 2.082784 [32064/50000]\n",
      "loss: 2.108678 [38464/50000]\n",
      "loss: 2.006403 [44864/50000]\n",
      "Epoch dataloader_64 5\n",
      "--------------\n",
      "loss: 2.090968 [   64/50000]\n",
      "loss: 2.042565 [ 6464/50000]\n",
      "loss: 1.970308 [12864/50000]\n",
      "loss: 2.079309 [19264/50000]\n",
      "loss: 1.997152 [25664/50000]\n",
      "loss: 2.037053 [32064/50000]\n",
      "loss: 2.086132 [38464/50000]\n",
      "loss: 1.945433 [44864/50000]\n",
      "Epoch dataloader_32 1\n",
      "--------------\n",
      "loss: 1.955454 [   32/50000]\n",
      "loss: 1.997043 [ 3232/50000]\n",
      "loss: 1.872851 [ 6432/50000]\n",
      "loss: 1.993511 [ 9632/50000]\n",
      "loss: 1.868830 [12832/50000]\n",
      "loss: 2.060363 [16032/50000]\n",
      "loss: 2.031751 [19232/50000]\n",
      "loss: 1.956437 [22432/50000]\n",
      "loss: 1.945372 [25632/50000]\n",
      "loss: 2.018635 [28832/50000]\n",
      "loss: 1.889557 [32032/50000]\n",
      "loss: 1.965294 [35232/50000]\n",
      "loss: 2.091008 [38432/50000]\n",
      "loss: 1.802140 [41632/50000]\n",
      "loss: 1.823775 [44832/50000]\n",
      "loss: 1.973033 [48032/50000]\n",
      "Epoch dataloader_32 2\n",
      "--------------\n",
      "loss: 1.903879 [   32/50000]\n",
      "loss: 1.954619 [ 3232/50000]\n",
      "loss: 1.773894 [ 6432/50000]\n",
      "loss: 1.921816 [ 9632/50000]\n",
      "loss: 1.714323 [12832/50000]\n",
      "loss: 1.938859 [16032/50000]\n",
      "loss: 1.958231 [19232/50000]\n",
      "loss: 1.858839 [22432/50000]\n",
      "loss: 1.912828 [25632/50000]\n",
      "loss: 1.917948 [28832/50000]\n",
      "loss: 1.830946 [32032/50000]\n",
      "loss: 1.896657 [35232/50000]\n",
      "loss: 2.050095 [38432/50000]\n",
      "loss: 1.749523 [41632/50000]\n",
      "loss: 1.786533 [44832/50000]\n",
      "loss: 1.947637 [48032/50000]\n",
      "Epoch dataloader_32 3\n",
      "--------------\n",
      "loss: 1.867899 [   32/50000]\n",
      "loss: 1.932507 [ 3232/50000]\n",
      "loss: 1.712644 [ 6432/50000]\n",
      "loss: 1.860366 [ 9632/50000]\n",
      "loss: 1.597536 [12832/50000]\n",
      "loss: 1.855730 [16032/50000]\n",
      "loss: 1.902385 [19232/50000]\n",
      "loss: 1.788799 [22432/50000]\n",
      "loss: 1.879992 [25632/50000]\n",
      "loss: 1.832700 [28832/50000]\n",
      "loss: 1.790760 [32032/50000]\n",
      "loss: 1.833834 [35232/50000]\n",
      "loss: 2.004264 [38432/50000]\n",
      "loss: 1.711541 [41632/50000]\n",
      "loss: 1.760221 [44832/50000]\n",
      "loss: 1.935796 [48032/50000]\n",
      "Epoch dataloader_32 4\n",
      "--------------\n",
      "loss: 1.829906 [   32/50000]\n",
      "loss: 1.911167 [ 3232/50000]\n",
      "loss: 1.659078 [ 6432/50000]\n",
      "loss: 1.808810 [ 9632/50000]\n",
      "loss: 1.516663 [12832/50000]\n",
      "loss: 1.797607 [16032/50000]\n",
      "loss: 1.863275 [19232/50000]\n",
      "loss: 1.749922 [22432/50000]\n",
      "loss: 1.845886 [25632/50000]\n",
      "loss: 1.764764 [28832/50000]\n",
      "loss: 1.756723 [32032/50000]\n",
      "loss: 1.778492 [35232/50000]\n",
      "loss: 1.961788 [38432/50000]\n",
      "loss: 1.673961 [41632/50000]\n",
      "loss: 1.740140 [44832/50000]\n",
      "loss: 1.931036 [48032/50000]\n",
      "Epoch dataloader_32 5\n",
      "--------------\n",
      "loss: 1.790929 [   32/50000]\n",
      "loss: 1.885289 [ 3232/50000]\n",
      "loss: 1.605425 [ 6432/50000]\n",
      "loss: 1.763452 [ 9632/50000]\n",
      "loss: 1.461298 [12832/50000]\n",
      "loss: 1.753010 [16032/50000]\n",
      "loss: 1.834715 [19232/50000]\n",
      "loss: 1.730236 [22432/50000]\n",
      "loss: 1.811000 [25632/50000]\n",
      "loss: 1.709897 [28832/50000]\n",
      "loss: 1.724445 [32032/50000]\n",
      "loss: 1.730509 [35232/50000]\n",
      "loss: 1.920089 [38432/50000]\n",
      "loss: 1.636480 [41632/50000]\n",
      "loss: 1.721242 [44832/50000]\n",
      "loss: 1.930243 [48032/50000]\n",
      "Time elapsed for batch_size of 64:54.365721\n",
      "Time elapsed for batch_size of 32:62.722892\n"
     ]
    }
   ],
   "source": [
    "# making sure it runs on gpu\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "# defining model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(3*32*32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# train loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Computer prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "# training loop with batch_size 64\n",
    "start_test1 = time.time() # save initial time\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch dataloader_64 {t+1}\\n--------------\")\n",
    "    train(train_dataloader_64, model, loss_fn, optimizer)\n",
    "end_test1 = time.time() # save end time\n",
    "\n",
    "# training loop with batch_size 32\n",
    "start_test2 = time.time()\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch dataloader_32 {t+1}\\n--------------\")\n",
    "    train(train_dataloader_32, model, loss_fn, optimizer)\n",
    "end_test2 = time.time()\n",
    "\n",
    "print(f\"Time elapsed for batch_size of 64:{end_test1-start_test1:5f}\")\n",
    "print(f\"Time elapsed for batch_size of 32:{end_test2-start_test2:5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "2.1 Modify the model to include an additional hidden layer with 128 neurons and a ReLU activation function. <br>\n",
    "2.2 Replace the ReLU activation function with another, such as LeakyReLU or Tanh. Compare the results. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define modified model\n",
    "class modifiedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(3*32*32, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model_modified = modifiedNN().to(device)\n",
    "\n",
    "\n",
    "# training\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "# test\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 2.320637 [   64/50000]\n",
      "loss: 2.311589 [ 6464/50000]\n",
      "loss: 2.297721 [12864/50000]\n",
      "loss: 2.314184 [19264/50000]\n",
      "loss: 2.308749 [25664/50000]\n",
      "loss: 2.306344 [32064/50000]\n",
      "loss: 2.304973 [38464/50000]\n",
      "loss: 2.296851 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 2.305395\n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 2.320637 [   64/50000]\n",
      "loss: 2.311589 [ 6464/50000]\n",
      "loss: 2.297721 [12864/50000]\n",
      "loss: 2.314184 [19264/50000]\n",
      "loss: 2.308749 [25664/50000]\n",
      "loss: 2.306344 [32064/50000]\n",
      "loss: 2.304973 [38464/50000]\n",
      "loss: 2.296851 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 2.305395\n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 2.320637 [   64/50000]\n",
      "loss: 2.311589 [ 6464/50000]\n",
      "loss: 2.297721 [12864/50000]\n",
      "loss: 2.314184 [19264/50000]\n",
      "loss: 2.308749 [25664/50000]\n",
      "loss: 2.306344 [32064/50000]\n",
      "loss: 2.304973 [38464/50000]\n",
      "loss: 2.296851 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 2.305395\n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 2.320637 [   64/50000]\n",
      "loss: 2.311589 [ 6464/50000]\n",
      "loss: 2.297721 [12864/50000]\n",
      "loss: 2.314184 [19264/50000]\n",
      "loss: 2.308749 [25664/50000]\n",
      "loss: 2.306344 [32064/50000]\n",
      "loss: 2.304973 [38464/50000]\n",
      "loss: 2.296851 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 2.305395\n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 2.320637 [   64/50000]\n",
      "loss: 2.311589 [ 6464/50000]\n",
      "loss: 2.297721 [12864/50000]\n",
      "loss: 2.314184 [19264/50000]\n",
      "loss: 2.308749 [25664/50000]\n",
      "loss: 2.306344 [32064/50000]\n",
      "loss: 2.304973 [38464/50000]\n",
      "loss: 2.296851 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 9.2%, Avg loss: 2.305395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "\n",
    "# comparing both modified and plain model\n",
    "\n",
    "# modified\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------\")\n",
    "    train(train_dataloader_64, model_modified, loss_fn, optimizer)\n",
    "    test(test_dataloader_64, model_modified, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 1.747504 [   64/50000]\n",
      "loss: 1.718490 [ 6464/50000]\n",
      "loss: 1.488520 [12864/50000]\n",
      "loss: 1.800623 [19264/50000]\n",
      "loss: 1.740206 [25664/50000]\n",
      "loss: 1.805503 [32064/50000]\n",
      "loss: 1.844375 [38464/50000]\n",
      "loss: 1.671431 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 40.1%, Avg loss: 1.713593\n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.721497 [   64/50000]\n",
      "loss: 1.693944 [ 6464/50000]\n",
      "loss: 1.467497 [12864/50000]\n",
      "loss: 1.786323 [19264/50000]\n",
      "loss: 1.723432 [25664/50000]\n",
      "loss: 1.790113 [32064/50000]\n",
      "loss: 1.825862 [38464/50000]\n",
      "loss: 1.658101 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 40.4%, Avg loss: 1.699543\n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.700364 [   64/50000]\n",
      "loss: 1.671583 [ 6464/50000]\n",
      "loss: 1.447907 [12864/50000]\n",
      "loss: 1.773197 [19264/50000]\n",
      "loss: 1.707784 [25664/50000]\n",
      "loss: 1.775854 [32064/50000]\n",
      "loss: 1.807949 [38464/50000]\n",
      "loss: 1.645206 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 40.8%, Avg loss: 1.686335\n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.681139 [   64/50000]\n",
      "loss: 1.651048 [ 6464/50000]\n",
      "loss: 1.429595 [12864/50000]\n",
      "loss: 1.760924 [19264/50000]\n",
      "loss: 1.693074 [25664/50000]\n",
      "loss: 1.762793 [32064/50000]\n",
      "loss: 1.790887 [38464/50000]\n",
      "loss: 1.633047 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 41.1%, Avg loss: 1.673953\n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.663368 [   64/50000]\n",
      "loss: 1.632558 [ 6464/50000]\n",
      "loss: 1.412449 [12864/50000]\n",
      "loss: 1.749789 [19264/50000]\n",
      "loss: 1.679057 [25664/50000]\n",
      "loss: 1.750684 [32064/50000]\n",
      "loss: 1.774529 [38464/50000]\n",
      "loss: 1.621351 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 41.4%, Avg loss: 1.662361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 5\n",
    "\n",
    "# comparing both modified and plain model\n",
    "\n",
    "# plain\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------\")\n",
    "    train(train_dataloader_64, model, loss_fn, optimizer)\n",
    "    test(test_dataloader_64, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do Modelo  \n",
    "\n",
    "3.1 Altere o número de épocas para 10 e observe como isso afeta a precisão do modelo.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 1.647398 [   64/50000]\n",
      "loss: 1.615947 [ 6464/50000]\n",
      "loss: 1.396326 [12864/50000]\n",
      "loss: 1.739592 [19264/50000]\n",
      "loss: 1.665690 [25664/50000]\n",
      "loss: 1.739096 [32064/50000]\n",
      "loss: 1.758780 [38464/50000]\n",
      "loss: 1.610207 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 1.651479\n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.632709 [   64/50000]\n",
      "loss: 1.600811 [ 6464/50000]\n",
      "loss: 1.381337 [12864/50000]\n",
      "loss: 1.730325 [19264/50000]\n",
      "loss: 1.653200 [25664/50000]\n",
      "loss: 1.728236 [32064/50000]\n",
      "loss: 1.743729 [38464/50000]\n",
      "loss: 1.599721 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 42.4%, Avg loss: 1.641216\n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.619340 [   64/50000]\n",
      "loss: 1.587145 [ 6464/50000]\n",
      "loss: 1.367351 [12864/50000]\n",
      "loss: 1.721909 [19264/50000]\n",
      "loss: 1.641701 [25664/50000]\n",
      "loss: 1.718011 [32064/50000]\n",
      "loss: 1.729756 [38464/50000]\n",
      "loss: 1.589820 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 42.8%, Avg loss: 1.631541\n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.607414 [   64/50000]\n",
      "loss: 1.574512 [ 6464/50000]\n",
      "loss: 1.354375 [12864/50000]\n",
      "loss: 1.714715 [19264/50000]\n",
      "loss: 1.631187 [25664/50000]\n",
      "loss: 1.708362 [32064/50000]\n",
      "loss: 1.716428 [38464/50000]\n",
      "loss: 1.580433 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 43.2%, Avg loss: 1.622388\n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.596566 [   64/50000]\n",
      "loss: 1.563142 [ 6464/50000]\n",
      "loss: 1.342079 [12864/50000]\n",
      "loss: 1.708578 [19264/50000]\n",
      "loss: 1.621436 [25664/50000]\n",
      "loss: 1.699237 [32064/50000]\n",
      "loss: 1.703897 [38464/50000]\n",
      "loss: 1.571771 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 43.4%, Avg loss: 1.613702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "# comparing how different epoch sizes affect the model\n",
    "# plain\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------\")\n",
    "    train(train_dataloader_64, model, loss_fn, optimizer)\n",
    "    test(test_dataloader_64, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 1.586795 [   64/50000]\n",
      "loss: 1.552444 [ 6464/50000]\n",
      "loss: 1.330564 [12864/50000]\n",
      "loss: 1.703293 [19264/50000]\n",
      "loss: 1.612178 [25664/50000]\n",
      "loss: 1.690472 [32064/50000]\n",
      "loss: 1.691673 [38464/50000]\n",
      "loss: 1.563648 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 43.8%, Avg loss: 1.605417\n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.577868 [   64/50000]\n",
      "loss: 1.542256 [ 6464/50000]\n",
      "loss: 1.319618 [12864/50000]\n",
      "loss: 1.698557 [19264/50000]\n",
      "loss: 1.603293 [25664/50000]\n",
      "loss: 1.681903 [32064/50000]\n",
      "loss: 1.679796 [38464/50000]\n",
      "loss: 1.555648 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 44.2%, Avg loss: 1.597474\n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.569596 [   64/50000]\n",
      "loss: 1.532812 [ 6464/50000]\n",
      "loss: 1.309147 [12864/50000]\n",
      "loss: 1.694107 [19264/50000]\n",
      "loss: 1.594513 [25664/50000]\n",
      "loss: 1.673611 [32064/50000]\n",
      "loss: 1.668659 [38464/50000]\n",
      "loss: 1.548197 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 44.5%, Avg loss: 1.589854\n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.561926 [   64/50000]\n",
      "loss: 1.523921 [ 6464/50000]\n",
      "loss: 1.298966 [12864/50000]\n",
      "loss: 1.689852 [19264/50000]\n",
      "loss: 1.585836 [25664/50000]\n",
      "loss: 1.665459 [32064/50000]\n",
      "loss: 1.657967 [38464/50000]\n",
      "loss: 1.541156 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 44.7%, Avg loss: 1.582528\n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.554765 [   64/50000]\n",
      "loss: 1.515565 [ 6464/50000]\n",
      "loss: 1.289213 [12864/50000]\n",
      "loss: 1.685578 [19264/50000]\n",
      "loss: 1.577296 [25664/50000]\n",
      "loss: 1.657739 [32064/50000]\n",
      "loss: 1.647847 [38464/50000]\n",
      "loss: 1.534332 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 1.575459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "# comparing how different epoch sizes affect the model\n",
    "# plain\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------\")\n",
    "    train(train_dataloader_64, model, loss_fn, optimizer)\n",
    "    test(test_dataloader_64, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "---------------------\n",
      "loss: 1.547964 [   64/50000]\n",
      "loss: 1.507553 [ 6464/50000]\n",
      "loss: 1.280073 [12864/50000]\n",
      "loss: 1.681130 [19264/50000]\n",
      "loss: 1.569011 [25664/50000]\n",
      "loss: 1.650088 [32064/50000]\n",
      "loss: 1.638507 [38464/50000]\n",
      "loss: 1.527525 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.2%, Avg loss: 1.568628\n",
      "\n",
      "Epoch 2\n",
      "---------------------\n",
      "loss: 1.541721 [   64/50000]\n",
      "loss: 1.499857 [ 6464/50000]\n",
      "loss: 1.271465 [12864/50000]\n",
      "loss: 1.676697 [19264/50000]\n",
      "loss: 1.560723 [25664/50000]\n",
      "loss: 1.642680 [32064/50000]\n",
      "loss: 1.629897 [38464/50000]\n",
      "loss: 1.520763 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.4%, Avg loss: 1.562034\n",
      "\n",
      "Epoch 3\n",
      "---------------------\n",
      "loss: 1.535698 [   64/50000]\n",
      "loss: 1.492443 [ 6464/50000]\n",
      "loss: 1.263055 [12864/50000]\n",
      "loss: 1.672385 [19264/50000]\n",
      "loss: 1.552816 [25664/50000]\n",
      "loss: 1.635186 [32064/50000]\n",
      "loss: 1.621818 [38464/50000]\n",
      "loss: 1.514438 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.5%, Avg loss: 1.555633\n",
      "\n",
      "Epoch 4\n",
      "---------------------\n",
      "loss: 1.530040 [   64/50000]\n",
      "loss: 1.485128 [ 6464/50000]\n",
      "loss: 1.255033 [12864/50000]\n",
      "loss: 1.667930 [19264/50000]\n",
      "loss: 1.545193 [25664/50000]\n",
      "loss: 1.627842 [32064/50000]\n",
      "loss: 1.614162 [38464/50000]\n",
      "loss: 1.508431 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 45.8%, Avg loss: 1.549395\n",
      "\n",
      "Epoch 5\n",
      "---------------------\n",
      "loss: 1.524474 [   64/50000]\n",
      "loss: 1.477935 [ 6464/50000]\n",
      "loss: 1.247428 [12864/50000]\n",
      "loss: 1.663319 [19264/50000]\n",
      "loss: 1.537906 [25664/50000]\n",
      "loss: 1.620587 [32064/50000]\n",
      "loss: 1.606906 [38464/50000]\n",
      "loss: 1.502654 [44864/50000]\n",
      "Test Error: \n",
      " Accuracy: 46.0%, Avg loss: 1.543370\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 50\n",
    "# comparing how different epoch sizes affect the model\n",
    "# plain\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n---------------------\")\n",
    "    train(train_dataloader_64, model, loss_fn, optimizer)\n",
    "    test(test_dataloader_64, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchtut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
